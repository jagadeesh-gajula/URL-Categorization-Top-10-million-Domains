{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "import html2text\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from tzlocal import get_localzone\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links=True\n",
    "pd.low_memory=False\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/VickyViper/Documents/URL categorization application/url-classification-dataset-dmoz/URL Classification.csv\")\n",
    "df = df.drop('1',axis=1)\n",
    "df.columns=['url','category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given with 1.5 million websites and they are divided into 25 parts to extract data.\n"
     ]
    }
   ],
   "source": [
    "def extract(url):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        if page.status_code == 200:\n",
    "            contents = page.content\n",
    "            soup = BeautifulSoup(contents)\n",
    "            [script.decompose() for script in soup([\"script\", \"style\"])]\n",
    "            strips = list(soup.stripped_strings)\n",
    "            strips = ' '.join(strips)\n",
    "            return strips\n",
    "        else:\n",
    "            return \"NULL\"\n",
    "    except:\n",
    "        return \"NULL\"\n",
    "    \n",
    "    \n",
    "def extract_1(url):\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(html)\n",
    "        data = soup.findAll(text=True)\n",
    "        def visible(element):\n",
    "            if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "                return False\n",
    "            elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "                return False\n",
    "            return True\n",
    "        result = filter(visible, data)\n",
    "        return ' '.join(result)\n",
    "    except:\n",
    "        return 'NULL'\n",
    "\n",
    "def extract_link(url):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "    except:\n",
    "        return \"NULL\"\n",
    "    data=page.read()\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    tags = ['title','a','p','input','div','html']\n",
    "    result = []\n",
    "    for k in tags:\n",
    "        fi=soup.find_all(k)\n",
    "        s = [h.handle(str(i)) for i in fi]\n",
    "        s = ' '.join(s)\n",
    "        s = s.replace('\\n',' ')\n",
    "        s = re.sub('[^A-Za-z0-9]+', ' ',s)\n",
    "        result.append(s)\n",
    "    return ' '.join(result)\n",
    "\n",
    "def prepare_url(url):\n",
    "    urls =[]\n",
    "    secure_scheme = 'https://'\n",
    "    insecure_scheme = 'http://'\n",
    "    netloc =  urlparse(url).netloc\n",
    "    if len(netloc) < 2:\n",
    "            netloc = urlparse(url).path\n",
    "    \n",
    "    urls.append(secure_scheme+netloc)\n",
    "    urls.append(insecure_scheme+netloc)\n",
    "    return urls\n",
    "\n",
    "def create_df(i):\n",
    "    url = prepare_url(df.url[i])\n",
    "    try:\n",
    "        text =  extract_link(url[0])\n",
    "        if text == 'NULL':\n",
    "            text = extract_link(url[1])\n",
    "        if text == 'NULL':\n",
    "            text = extract(url[0])\n",
    "        if text == 'NULL':\n",
    "            text = extract(url[1])\n",
    "        if text == 'NULL':\n",
    "            text = extract_1(url[0])\n",
    "        if text == 'NULL':\n",
    "            text = extract_1(url[1])\n",
    "        data.loc[i] = [df.url[i],df.category[i],text]\n",
    "    except:\n",
    "        data.loc[i] = [df.url[i],df.category[i],\"NULL\"]\n",
    "    print((i/62519)*100,\" % complete\",end='\\r')\n",
    "\n",
    "\n",
    "data = pd.DataFrame(columns=['url','category','text'])\n",
    "print(\"You are given with 1.56 million websites and they are divided into 25 parts to extract data.\")\n",
    "section = int(df.shape[0]/25)\n",
    "part = int(input(\"please type any number 0 - 24 to finish\"))\n",
    "begin = section * part\n",
    "end =  begin + section\n",
    "print(\"You part is: \",part,\" so the range is \",begin,\"-\",end)\n",
    "format = \"%Y-%m-%d %H:%M:%S\"\n",
    "now_utc = datetime.now(timezone('GMT'))\n",
    "now_local = now_utc.astimezone(get_localzone())\n",
    "print(\"Program started at\",now_local.strftime(format))\n",
    "\n",
    "for i in range(begin,end):\n",
    "        while threading.active_count() > 100:\n",
    "            time.sleep(1)\n",
    "        t =  threading.Thread(target=create_df,args=(i,))\n",
    "        t.start()\n",
    "\n",
    "while data.shape[0] < section-10:\n",
    "        time.sleep(1)\n",
    "        \n",
    "now_utc = datetime.now(timezone('GMT'))\n",
    "now_local = now_utc.astimezone(get_localzone())\n",
    "print(\"Program finished at\",now_local.strftime(format))\n",
    "\n",
    "print(\"saving data to CSV file\")\n",
    "data.to_csv(str(part)+\"_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
