{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.low_memory=False\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"URL Classification.csv\")\n",
    "df = df.drop('1',axis=1)\n",
    "df.columns=['url','category']\n",
    "data = pd.DataFrame(columns=['url','category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.category.unique().tolist()\n",
    "for i in categories:\n",
    "    index_list = df[df.category == i ].index.tolist()\n",
    "    x = random.sample(index_list,5000)\n",
    "    data = pd.concat([df.loc[x],data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('sample_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://users.belgacom.net/hoshinkido.belgium</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://gobearcats.cstv.com/sports/w-baskbl/cin...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.hongluck.org</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://aikithai.tripod.com/</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.decathlon2000.ee/eng/index.php</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>http://www.sweet18teen.com/xxxhentai/</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>http://www.zieberts.com/</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>http://www.unprivacy.com/</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>http://www.sex-xxx-transexuals.com/sites/porn/...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>http://www.black-woman.net/andy-asian-teens/</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url category\n",
       "0           http://users.belgacom.net/hoshinkido.belgium   Sports\n",
       "1      http://gobearcats.cstv.com/sports/w-baskbl/cin...   Sports\n",
       "2                                http://www.hongluck.org   Sports\n",
       "3                            http://aikithai.tripod.com/   Sports\n",
       "4              http://www.decathlon2000.ee/eng/index.php   Sports\n",
       "...                                                  ...      ...\n",
       "74995              http://www.sweet18teen.com/xxxhentai/    Adult\n",
       "74996                           http://www.zieberts.com/    Adult\n",
       "74997                          http://www.unprivacy.com/    Adult\n",
       "74998  http://www.sex-xxx-transexuals.com/sites/porn/...    Adult\n",
       "74999       http://www.black-woman.net/andy-asian-teens/    Adult\n",
       "\n",
       "[75000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import threading \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "import html2text\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from tzlocal import get_localzone\n",
    "import threading\n",
    "import time\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "import random\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links=True\n",
    "pd.low_memory=False\n",
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "df.columns=['url','category']\n",
    "data = pd.DataFrame(columns=['url','category','text'])\n",
    "avg=[]\n",
    "\n",
    "\n",
    "def stop_words_remover(word_tokens): \n",
    "    filtered_sent = [w for w in word_tokens if not w in stop_words] \n",
    "    final = ' '.join(filtered_sent)\n",
    "    return final\n",
    "\n",
    "def clean_stopwords(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    return ' '.join(tokens_without_sw)\n",
    "\n",
    "\n",
    "def extract(url):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        if page.status_code == 200:\n",
    "            contents = page.content\n",
    "            soup = BeautifulSoup(contents)\n",
    "            [script.decompose() for script in soup([\"script\", \"style\"])]\n",
    "            strips = list(soup.stripped_strings)\n",
    "            strips = ' '.join(strips)\n",
    "            strips = strips.replace(',',' ')\n",
    "            return clean_stopwords(strips)\n",
    "        else:\n",
    "            return \"NULL\"\n",
    "    except:\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "def extract_link(url):\n",
    "    page = urllib.request.urlopen(url)\n",
    "    data=page.read()\n",
    "    soup=BeautifulSoup(data,'html.parser',from_encoding=\"iso-8859-1\")\n",
    "    tags = ['title','a','p','input','div','html','table']\n",
    "    result = []\n",
    "    if len(tags) == 0:\n",
    "        return \"NULL\"\n",
    "    for k in tags:\n",
    "        fi=soup.find_all(k)\n",
    "        s = [h.handle(str(i)) for i in fi]\n",
    "        s = ' '.join(s)\n",
    "        s = s.replace('\\n',' ')\n",
    "        s = s.replace(',',' ')\n",
    "        result.append(s)\n",
    "    final_sent = ' '.join(result)\n",
    "    return clean_stopwords(final_sent)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_url(url):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    urls =[]\n",
    "    secure_scheme = 'https://'\n",
    "    insecure_scheme = 'http://'\n",
    "    netloc =  urlparse(url).netloc\n",
    "    if len(netloc) < 2:\n",
    "            netloc = urlparse(url).path\n",
    "    \n",
    "    urls.append(secure_scheme+netloc)\n",
    "    urls.append(insecure_scheme+netloc)\n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extract_link(\"http://www.youtube.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
